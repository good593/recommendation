{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Word2vec](https://www.kaggle.com/code/chocozzz/00-word2vec-1)\n",
    "- https://lovit.github.io/nlp/representation/2018/03/26/word_doc_embedding/\n",
    "\n",
    "### 통계기반의 방법 단점; TF-IDF\n",
    "- 대규모 말뭉치를 다룰 때 메모리상의 문제가 발생\n",
    "  - 높은 차원을 가짐, 매우 sparse한 형태의 데이터임\n",
    "  - 예) 100만개의 문서를 다루는 경우: 100만개의 문서에 등장한 모든 단어를 추출해야하고 이때 단어의 수는 1문서당 새로운 단어가 10개면, 1000만개 정도의 말뭉치가 형성됨\n",
    "- 한번에 학습 데이터 전체를 진행함\n",
    "  - 큰 작업을 처리하기 어려움\n",
    "  - GPU와 같은 병렬처리를 기대하기 힘듬\n",
    "- 학습을 통해서 개선하기가 어려움  \n",
    "\n",
    "### Word2vec\n",
    "> 단어간 유사도를 반영하여 단어를 벡터로 바꿔주는 임베딩 방법론입니다.   \n",
    "> 원-핫벡터 형태의 sparse matrix이 가지는 단점을 해소하고자 저차원의 공간에 벡터로 매핑하는 것이 특징입니다.   \n",
    "> Word2vec은 '비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다'라는 가정을 통해서 학습을 진행합니다. 저차원에 학습된 단어의 의미를 분산하여 표현하기에 단어 간 유사도를 계산할 수 있습니다.   \n",
    "- CBOW\n",
    "  - 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법입니다.\n",
    "- Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법입니다.   \n",
    "> -> 일반적으로 CBOW보다 Skip-Gram이 성능이 더 좋음.\n",
    "  \n",
    "#### 추천시스템에서는 단어를 구매 상품으로 바꿔서 구매한 페턴에 Word2vec을 적용해서 비슷한 상품을 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/movies/\"\n",
    "\n",
    "movie = pd.read_csv(path + 'ratings.csv', low_memory=False)\n",
    "movie.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>383</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>789652009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>789652009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383</td>\n",
       "      <td>1079</td>\n",
       "      <td>3.0</td>\n",
       "      <td>789652009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>409</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>828212412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>409</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>828212412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0     383       21     3.0  789652009\n",
       "1     383       47     5.0  789652009\n",
       "2     383     1079     3.0  789652009\n",
       "3     409       21     5.0  828212412\n",
       "4     409       25     4.0  828212412"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie = movie.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "\n",
       "                               homepage    id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story   862  tt0114709                en   \n",
       "1                                   NaN  8844  tt0113497                en   \n",
       "\n",
       "  original_title                                           overview  ...  \\\n",
       "0      Toy Story  Led by Woody, Andy's toys live happily in his ...  ...   \n",
       "1        Jumanji  When siblings Judy and Peter discover an encha...  ...   \n",
       "\n",
       "  release_date      revenue runtime  \\\n",
       "0   1995-10-30  373554033.0    81.0   \n",
       "1   1995-12-15  262797249.0   104.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                                     tagline      title  video vote_average  \\\n",
       "0                                        NaN  Toy Story  False          7.7   \n",
       "1  Roll the dice and unleash the excitement!    Jumanji  False          6.9   \n",
       "\n",
       "  vote_count  \n",
       "0     5415.0  \n",
       "1     2413.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화의 Metadata를 불러와서 movieID에 맞는 TITLE을 구해줍니다. \n",
    "meta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\n",
    "meta.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
       "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
       "       'popularity', 'poster_path', 'production_companies',\n",
       "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
       "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
       "       'vote_average', 'vote_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.rename(columns={'id':'movieId'})\n",
    "movie['movieId'] = movie['movieId'].astype(str)\n",
    "meta['movieId'] = meta['movieId'].astype(str)\n",
    "\n",
    "movie = pd.merge(movie, meta[['movieId', 'original_title']], how='left', on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserId 별 MovidID 구매 목록을 생성 \n",
    "# item_agg = movie.groupby(['title'])['userId'].agg({'nunique'}).reset_index() \n",
    "# item_agg = item_agg[item_agg['nunique'] >= 5]['title'].values \n",
    "# user_agg = movie.groupby(['userId'])['title'].agg({'nunique'}).reset_index() \n",
    "# user_agg = user_agg[user_agg['nunique'] >= 5]['userId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 수행 \n",
    "# 성능향상을 위해 5회 미만 구매된 상품, 5회 미만 구매한 고객은 제외하고 분석진행 \n",
    "# movie['check1'] = 0 \n",
    "# movie.loc[movie['title'].isin(item_agg), 'check1'] = 1 \n",
    "# movie['check2'] = 0 \n",
    "# movie.loc[movie['userId'].isin(user_agg), 'check2'] = 1 \n",
    "# movie = movie.loc[(movie['check1'] == 1) & (movie['check2'] == 1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = movie[movie['original_title'].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Jay and Silent Bob Strike Back, Vivement dima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Terminator 3: Rise of the Machines, The Conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[300, The Killing, Shortbus, Finding Neverland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[David, The Wedding Planner, Casablanca, Sleep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Gleaming the Cube, Cool Hand Luke, Hidalgo, U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   unique\n",
       "userId                                                   \n",
       "1       [Jay and Silent Bob Strike Back, Vivement dima...\n",
       "2       [Terminator 3: Rise of the Machines, The Conve...\n",
       "3       [300, The Killing, Shortbus, Finding Neverland...\n",
       "4       [David, The Wedding Planner, Casablanca, Sleep...\n",
       "5       [Gleaming the Cube, Cool Hand Luke, Hidalgo, U..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = movie.groupby(['userId'])['original_title'].agg({'unique'})\n",
    "agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Endless Summer', 'Jarhead', '彼女の想いで', ...,\n",
       "       'The Lonedale Operator', 'Violeta se fue a los cielos',\n",
       "       'To Kill a Priest'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie['original_title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word2vec 함수](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "### input params\n",
    "- vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "- window (int, optional) – Maximum distance between the current and predicted word within a sentence.  \n",
    "> 주변을 몇 간까지 볼지에 대한 크기\n",
    "- min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "- workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines\n",
    "- sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "- epochs (int, optional) – Number of iterations (epochs) over the corpus. (Formerly: iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int형식은 Word2vec에서 학습이 안되어서 String으로 변경해줍니다. \n",
    "sentence = []\n",
    "for user_sentence in agg['unique'].values:\n",
    "    sentence.append(list(map(str, user_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec의 학습을 진행해줍니다. \n",
    "from gensim.models import Word2Vec\n",
    "embedding_model = Word2Vec(sentence, vector_size=20, window = 5, \n",
    "                          min_count=1, workers=4, epochs=200, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Snow Cake', 0.855394184589386),\n",
       " (\"L'Aile ou la Cuisse\", 0.8079094886779785),\n",
       " ('Domicile Conjugal', 0.7673839926719666),\n",
       " ('Conquest of the Planet of the Apes', 0.7452538013458252),\n",
       " ('Star Trek: Nemesis', 0.7430431842803955),\n",
       " ('Heavenly Creatures', 0.7352131605148315),\n",
       " ('Forrest Gump', 0.7338284850120544),\n",
       " ('Face/Off', 0.7298555970191956),\n",
       " ('Rumor Has It...', 0.7277589440345764),\n",
       " ('Mr. Brooks', 0.7254560589790344)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.wv.most_similar(positive=['Spider-Man 2'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\n",
    "meta = meta[meta['original_title'].notnull()].reset_index(drop=True)\n",
    "meta = meta[meta['overview'].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f48c3ff381745b8bd8abfaa9be0ec43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "overview = []\n",
    "for words in tqdm(meta['overview']):\n",
    "    word_tokens = word_tokenize(words)\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', str(word_tokens))\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    result = ''\n",
    "    for token in sentence_tokens: \n",
    "        if token not in stop_words:\n",
    "            result += ' ' + token \n",
    "    result = result.strip().lower()\n",
    "    overview.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['pre_overview'] = overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectorizer = doc2vec.Doc2Vec(\n",
    "    dm=0,            # PV-DBOW / default 1\n",
    "    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n",
    "    window=10,        # distance between the predicted word and context words\n",
    "    vector_size=100,        # vector size\n",
    "    alpha=0.025,     # learning-rate\n",
    "    seed=1234,\n",
    "    min_count=5,    # ignore with freq lower\n",
    "    min_alpha=0.025, # min learning-rate\n",
    "    workers=4,   # multi cpu\n",
    "    hs = 1,          # hierar chical softmax / default 0\n",
    "    negative = 10   # negative sampling / default 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "agg = meta[['id', 'original_title', 'pre_overview']]\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument((c), [d]) for d, c in agg[['original_title', 'pre_overview']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d100,n10,hs,w10,mc5,s0.001,t4>\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "print(str(doc_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf174fa68b3545579cfd70be574c0700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During Time: 502.68213391304016\n"
     ]
    }
   ],
   "source": [
    "# 벡터 문서 학습\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.epochs)\n",
    "    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n",
    "\n",
    "#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.epochs)\n",
    "end = time()\n",
    "print(\"During Time: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Letzte Worte', 0.7219405174255371),\n",
       " ('It Stains the Sands Red', 0.7019785046577454),\n",
       " ('El vendedor de humo', 0.6761186122894287),\n",
       " ('Особенности национальной политики', 0.6540822386741638),\n",
       " ('Children in the Surf at Coney Island', 0.6527005434036255),\n",
       " ('Der Sandmann', 0.6481742858886719),\n",
       " ('The Aristocats', 0.6475422382354736),\n",
       " ('La moutarde me monte au nez', 0.6467899084091187),\n",
       " ('По следам бременских музыкантов', 0.6384307146072388),\n",
       " (\"Independents' Day\", 0.6233192682266235),\n",
       " ('Kader', 0.6233088970184326),\n",
       " ('Meet Me in Venice', 0.6139765977859497),\n",
       " ('Begegnung mit Fritz Lang', 0.61335688829422),\n",
       " ('Live Forever as You Are Now with Alan Resnick', 0.6125729084014893),\n",
       " ('Killer: A Journal of Murder', 0.6118432879447937),\n",
       " ('The Unbearable Lightness of Being', 0.6092782616615295),\n",
       " ('Skazka o Poteryannom Vremeni', 0.6004007458686829),\n",
       " ('Samoubiytsy', 0.5998007655143738),\n",
       " ('活着', 0.5949916839599609),\n",
       " ('Thunder Road', 0.5943598747253418)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.docvecs.most_similar('Toy Story', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Great Ecstasy of Robert Carmichael', 0.669013261795044),\n",
       " ('Just Like Us', 0.634733259677887),\n",
       " ('Der Räuber', 0.6138728857040405),\n",
       " ('我知女人心', 0.6040328145027161),\n",
       " ('Se sei vivo spara', 0.6016620397567749),\n",
       " ('Tomorrow, When the War Began', 0.5776289701461792),\n",
       " ('Blood River', 0.5749691128730774),\n",
       " ('Cold Weather', 0.5611975789070129),\n",
       " ('Classe tous risques', 0.5587496161460876),\n",
       " ('Above and Beyond', 0.5571764707565308),\n",
       " ('Fear and Desire', 0.554139256477356),\n",
       " ('Emmas Glück', 0.5541350841522217),\n",
       " ('The Black Rose', 0.5459890961647034),\n",
       " ('Kasaba', 0.5439414381980896),\n",
       " ('Torch Song', 0.5344892740249634),\n",
       " ('Demonic Toys', 0.5337750911712646),\n",
       " ('A londoni férfi', 0.529091477394104),\n",
       " ('Zamilované Maso', 0.5248921513557434),\n",
       " ('Tribute', 0.5182867050170898),\n",
       " ('Ménilmontant', 0.5175023078918457)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.docvecs.most_similar('Harry Potter and the Deathly Hallows: Part 1', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "819822aad1d7dd5a8911f6442f86af1dcc8bcd014a1dc82c30af569169ee7dac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('pytorch3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
