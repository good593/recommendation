{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b724976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import struct\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AvazuDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path=None, cache_path='avazu/.avazu', rebuild_cache=True, min_threshold=1):\n",
    "        self.NUM_FEATS = 22\n",
    "        self.min_threshold = min_threshold\n",
    "        if rebuild_cache or not Path(cache_path).exists():\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)\n",
    "            if dataset_path is None:\n",
    "                raise ValueError('create cache: failed: dataset_path is None')\n",
    "            self.__build_cache(dataset_path, cache_path)\n",
    "        self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = txn.stat()['entries'] - 1\n",
    "            self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            np_array = np.frombuffer(\n",
    "                txn.get(struct.pack('>I', index)), dtype=np.uint).astype(dtype=np.int_)\n",
    "        return np_array[1:], np_array[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __build_cache(self, path, cache_path):\n",
    "        feat_mapper, defaults = self.__get_feat_mapper(path)\n",
    "        with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
    "            field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint)\n",
    "            for i, fm in feat_mapper.items():\n",
    "                field_dims[i - 1] = len(fm) + 1\n",
    "            with env.begin(write=True) as txn:\n",
    "                txn.put(b'field_dims', field_dims.tobytes())\n",
    "            for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
    "                with env.begin(write=True) as txn:\n",
    "                    for key, value in buffer:\n",
    "                        txn.put(key, value)\n",
    "\n",
    "    def __get_feat_mapper(self, path):\n",
    "        feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "        with open(path, encoding='latin1') as f:\n",
    "            f.readline()\n",
    "            pbar = tqdm.tqdm(f, mininterval=1, smoothing=0.1)\n",
    "            pbar.set_description('Create avazu dataset cache: counting features')\n",
    "            for line in pbar:\n",
    "                values = line.rstrip('\\n').split(',')\n",
    "                if len(values) != self.NUM_FEATS + 2:\n",
    "                    continue\n",
    "                for i in range(1, self.NUM_FEATS + 1):\n",
    "                    feat_cnts[i][values[i + 1]] += 1\n",
    "        feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()}\n",
    "        feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
    "        defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
    "        print(feat_mapper, defaults)\n",
    "        return feat_mapper, defaults\n",
    "\n",
    "    def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
    "        item_idx = 0\n",
    "        buffer = list()\n",
    "        with open(path ,encoding='latin1') as f:\n",
    "            f.readline()\n",
    "            pbar = tqdm.tqdm(f, mininterval=1, smoothing=0.1)\n",
    "            pbar.set_description('Create avazu dataset cache: setup lmdb')\n",
    "            for line in pbar:\n",
    "                values = line.rstrip('\\n').split(',')\n",
    "                if len(values) != self.NUM_FEATS + 2:\n",
    "                    continue\n",
    "                np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint)\n",
    "                np_array[0] = int(values[1])\n",
    "                for i in range(1, self.NUM_FEATS + 1):\n",
    "                    np_array[i] = feat_mapper[i].get(values[i+1], defaults[i])\n",
    "                buffer.append((struct.pack('>I', item_idx), np_array.tobytes()))\n",
    "                item_idx += 1\n",
    "                if item_idx % buffer_size == 0:\n",
    "                    yield buffer\n",
    "                    buffer.clear()\n",
    "            yield buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d64e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class MovieLens20MDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, sep=',', engine='c', header='infer'):\n",
    "        data = pd.read_csv(dataset_path, sep=sep, engine=engine, header=header).to_numpy()[:, :3]\n",
    "        self.items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
    "        self.targets = self.__preprocess_target(data[:, 2]).astype(np.float32)\n",
    "        self.field_dims = np.max(self.items, axis=0) + 1\n",
    "        self.user_field_idx = np.array((0, ), dtype=np.long)\n",
    "        self.item_field_idx = np.array((1,), dtype=np.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index], self.targets[index]\n",
    "\n",
    "    def __preprocess_target(self, target):\n",
    "        target[target <= 3] = 0\n",
    "        target[target > 3] = 1\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ff2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesLinear(torch.nn.Module):\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int_)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "    \n",
    "class FieldAwareFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim=4):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int_)\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n",
    "        ix = list()\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "    \n",
    "class FieldAwareFactorizationMachineModel(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
    "        x = self.linear(x) + ffm_term\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a6ac72c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create avazu dataset cache: counting features: : 8223836it [00:08, 1015674.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create avazu dataset cache: setup lmdb: : 8223836it [00:08, 1025121.61it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 98>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--save_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchkpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    111\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m--> 112\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m     \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(dataset_name, dataset_path, model_name, epoch, learning_rate, batch_size, weight_decay, device, save_dir)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m     89\u001b[0m     train(model, optimizer, train_data_loader, criterion, device)\n\u001b[1;32m---> 90\u001b[0m     auc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch_i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation: auc:\u001b[39m\u001b[38;5;124m'\u001b[39m, auc)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m early_stopper\u001b[38;5;241m.\u001b[39mis_continuable(model, auc):\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     66\u001b[0m         targets\u001b[38;5;241m.\u001b[39mextend(target\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     67\u001b[0m         predicts\u001b[38;5;241m.\u001b[39mextend(y\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:547\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03mfrom prediction scores.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03marray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    546\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    551\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    552\u001b[0m ):\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:909\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 909\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    916\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataset(name, path):\n",
    "    if name == 'avazu':\n",
    "        return AvazuDataset(path)\n",
    "    elif name == 'movie':\n",
    "        return MovieLens20MDataset(path)\n",
    "    else:\n",
    "        raise ValueError('unknown dataset name: ' + name)\n",
    "\n",
    "\n",
    "def get_model(name, dataset):\n",
    "    field_dims = dataset.field_dims\n",
    "    if name == 'ffm':\n",
    "        return FieldAwareFactorizationMachineModel(field_dims, embed_dim=4)\n",
    "    else:\n",
    "        raise ValueError('unknown model name: ' + name)\n",
    "\n",
    "\n",
    "class EarlyStopper(object):\n",
    "    def __init__(self, num_trials, save_path):\n",
    "        self.num_trials = num_trials\n",
    "        self.trial_counter = 0\n",
    "        self.best_accuracy = 0\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def is_continuable(self, model, accuracy):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.trial_counter = 0\n",
    "            torch.save(model, self.save_path)\n",
    "            return True\n",
    "        elif self.trial_counter + 1 < self.num_trials:\n",
    "            self.trial_counter += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "        \n",
    "def train(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (fields, target) in tk0:\n",
    "        fields, target = fields.to(device), target.to(device)\n",
    "        y = model(fields)\n",
    "        loss = criterion(y, target.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss=total_loss / log_interval)\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for fields, target in tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0):\n",
    "            fields, target = fields.to(device), target.to(device)\n",
    "            y = model(fields)\n",
    "            targets.extend(target.tolist())\n",
    "            predicts.extend(y.tolist())\n",
    "    return roc_auc_score(targets, predicts)\n",
    "\n",
    "\n",
    "def main(dataset_name, dataset_path, model_name, epoch, learning_rate, batch_size, weight_decay, device, save_dir):\n",
    "    device = torch.device(device)\n",
    "    dataset = get_dataset(dataset_name, dataset_path)\n",
    "    \n",
    "    train_length = int(len(dataset) * 0.8)\n",
    "    valid_length = int(len(dataset) * 0.1)\n",
    "    test_length = len(dataset) - train_length - valid_length\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, (train_length, valid_length, test_length))\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=0)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    model = get_model(model_name, dataset).to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    early_stopper = EarlyStopper(num_trials=2, save_path=f'{save_dir}/{model_name}.pt')\n",
    "    for epoch_i in range(epoch):\n",
    "        train(model, optimizer, train_data_loader, criterion, device)\n",
    "        auc = test(model, valid_data_loader, device)\n",
    "        print('epoch:', epoch_i, 'validation: auc:', auc)\n",
    "        if not early_stopper.is_continuable(model, auc):\n",
    "            print(f'validation: best auc: {early_stopper.best_accuracy}')\n",
    "            break\n",
    "    auc = test(model, test_data_loader, device)\n",
    "    print(f'test auc: {auc}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset_name', default='movie')\n",
    "    parser.add_argument('--dataset_path', default='ml-20m/ratings.csv')\n",
    "    parser.add_argument('--model_name', default='ffm')\n",
    "    parser.add_argument('--epoch', type=int, default=100)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
    "    parser.add_argument('--device', default='cpu')\n",
    "    parser.add_argument('--save_dir', default='chkpt')\n",
    "    args = parser.parse_args(args=[])\n",
    "    main(args.dataset_name,\n",
    "         args.dataset_path,\n",
    "         args.model_name,\n",
    "         args.epoch,\n",
    "         args.learning_rate,\n",
    "         args.batch_size,\n",
    "         args.weight_decay,\n",
    "         args.device,\n",
    "         args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cf3a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('avazu/train.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30793e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('pytorch3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "819822aad1d7dd5a8911f6442f86af1dcc8bcd014a1dc82c30af569169ee7dac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
