{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import struct\n",
    "# from collections import defaultdict\n",
    "# from pathlib import Path\n",
    "\n",
    "# import lmdb\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.utils.data\n",
    "# from tqdm import tqdm\n",
    "# import gzip\n",
    "\n",
    "# class AvazuDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset_path=None, cache_path='.avazu', rebuild_cache=False, min_threshold=4):\n",
    "#         self.NUM_FEATS = 22\n",
    "#         self.min_threshold = min_threshold\n",
    "#         if rebuild_cache or not Path(cache_path).exists():\n",
    "#             shutil.rmtree(cache_path, ignore_errors=True)\n",
    "#             if dataset_path is None:\n",
    "#                 raise ValueError('create cache: failed: dataset_path is None')\n",
    "#             self.__build_cache(dataset_path, cache_path)\n",
    "#         self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             self.length = txn.stat()['entries'] - 1\n",
    "#             self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             np_array = np.frombuffer(\n",
    "#                 txn.get(struct.pack('>I', index)), dtype=np.uint).astype(dtype=np.int_)\n",
    "#         return np_array[1:], np_array[0]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def __build_cache(self, path, cache_path):\n",
    "#         feat_mapper, defaults = self.__get_feat_mapper(path)\n",
    "#         with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
    "#             field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint)\n",
    "#             for i, fm in feat_mapper.items():\n",
    "#                 field_dims[i - 1] = len(fm) + 1\n",
    "#             with env.begin(write=True) as txn:\n",
    "#                 txn.put(b'field_dims', field_dims.tobytes())\n",
    "#             for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
    "#                 with env.begin(write=True) as txn:\n",
    "#                     for key, value in buffer:\n",
    "#                         txn.put(key, value)\n",
    "\n",
    "#     def __get_feat_mapper(self, path):\n",
    "#         feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "#         with gzip.open(path, 'rt') as f:\n",
    "#             f.readline()\n",
    "#             pbar = tqdm.tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create avazu dataset cache: counting features')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split(',')\n",
    "#                 if len(values) != self.NUM_FEATS + 2:\n",
    "#                     continue\n",
    "#                 for i in range(1, self.NUM_FEATS + 1):\n",
    "#                     feat_cnts[i][values[i + 1]] += 1\n",
    "#         feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()}\n",
    "#         feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
    "#         defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
    "#         # print(feat_mapper, defaults)\n",
    "#         return feat_mapper, defaults\n",
    "\n",
    "#     def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
    "#         item_idx = 0\n",
    "#         buffer = list()\n",
    "#         with gzip.open(path, 'rt') as f:\n",
    "#             f.readline()\n",
    "#             pbar = tqdm.tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create avazu dataset cache: setup lmdb')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split(',')\n",
    "#                 if len(values) != self.NUM_FEATS + 2:\n",
    "#                     continue\n",
    "#                 np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint)\n",
    "#                 np_array[0] = int(values[1])\n",
    "#                 for i in range(1, self.NUM_FEATS + 1):\n",
    "#                     np_array[i] = feat_mapper[i].get(values[i+1], defaults[i])\n",
    "#                 buffer.append((struct.pack('>I', item_idx), np_array.tobytes()))\n",
    "#                 item_idx += 1\n",
    "#                 if item_idx % buffer_size == 0:\n",
    "#                     yield buffer\n",
    "#                     buffer.clear()\n",
    "#             yield buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int_)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "    \n",
    "class FieldAwareFactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim=4):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int_)\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n",
    "        ix = list()\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "    \n",
    "class FieldAwareFactorizationMachineModel(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
    "        x = self.linear(x) + ffm_term\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  40428967\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test avazu.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=111'>112</a>\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--save_dir\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mavazu/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=112'>113</a>\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args(args\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=114'>115</a>\u001b[0m main(args\u001b[39m.\u001b[39;49mdataset_name,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=115'>116</a>\u001b[0m     args\u001b[39m.\u001b[39;49mdataset_path,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=116'>117</a>\u001b[0m     args\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=117'>118</a>\u001b[0m     args\u001b[39m.\u001b[39;49mepoch,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=118'>119</a>\u001b[0m     args\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=119'>120</a>\u001b[0m     args\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=120'>121</a>\u001b[0m     args\u001b[39m.\u001b[39;49mweight_decay,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=121'>122</a>\u001b[0m     args\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=122'>123</a>\u001b[0m     args\u001b[39m.\u001b[39;49msave_dir)\n",
      "\u001b[1;32m/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test avazu.ipynb 셀 3\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_name, dataset_path, model_name, epoch, learning_rate, batch_size, weight_decay, device, save_dir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=83'>84</a>\u001b[0m valid_data_loader \u001b[39m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=84'>85</a>\u001b[0m test_data_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=85'>86</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(model_name, dataset)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=86'>87</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=87'>88</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;32m/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test avazu.ipynb 셀 3\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, dataset)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=13'>14</a>\u001b[0m field_dims \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mfield_dims\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mffm\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m FieldAwareFactorizationMachineModel(field_dims, embed_dim\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=16'>17</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=17'>18</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munknown model name: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name)\n",
      "\u001b[1;32m/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test avazu.ipynb 셀 3\u001b[0m in \u001b[0;36mFieldAwareFactorizationMachineModel.__init__\u001b[0;34m(self, field_dims, embed_dim)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, field_dims, embed_dim):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=34'>35</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=35'>36</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear \u001b[39m=\u001b[39m FeaturesLinear(field_dims)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=36'>37</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffm \u001b[39m=\u001b[39m FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
      "\u001b[1;32m/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test avazu.ipynb 셀 3\u001b[0m in \u001b[0;36mFeaturesLinear.__init__\u001b[0;34m(self, field_dims, output_dim)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, field_dims, output_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mEmbedding(\u001b[39msum\u001b[39;49m(field_dims), output_dim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros((output_dim,)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyoungwon-cho/dev/github/recommendation/pytorch-fm/test%20avazu.ipynb#ch0000012?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffsets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray((\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mcumsum(field_dims)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint_)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9-dev/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/modules/sparse.py:139\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq \u001b[39m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataset(name, path):\n",
    "    if name == 'avazu':\n",
    "        return AvazuDataset(path)\n",
    "    else:\n",
    "        raise ValueError('unknown dataset name: ' + name)\n",
    "\n",
    "\n",
    "def get_model(name, dataset):\n",
    "    field_dims = dataset.field_dims\n",
    "    if name == 'ffm':\n",
    "        return FieldAwareFactorizationMachineModel(field_dims, embed_dim=4)\n",
    "    else:\n",
    "        raise ValueError('unknown model name: ' + name)\n",
    "\n",
    "\n",
    "class EarlyStopper(object):\n",
    "    def __init__(self, num_trials, save_path):\n",
    "        self.num_trials = num_trials\n",
    "        self.trial_counter = 0\n",
    "        self.best_accuracy = 0\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def is_continuable(self, model, accuracy):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.trial_counter = 0\n",
    "            torch.save(model, self.save_path)\n",
    "            return True\n",
    "        elif self.trial_counter + 1 < self.num_trials:\n",
    "            self.trial_counter += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "        \n",
    "def train(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (fields, target) in enumerate(tk0):\n",
    "        fields, target = fields.to(device), target.to(device)\n",
    "        y = model(fields)\n",
    "        loss = criterion(y, target.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss=total_loss / log_interval)\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for fields, target in tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0):\n",
    "            fields, target = fields.to(device), target.to(device)\n",
    "            y = model(fields)\n",
    "            targets.extend(target.tolist())\n",
    "            predicts.extend(y.tolist())\n",
    "    return roc_auc_score(targets, predicts)\n",
    "\n",
    "\n",
    "def main(dataset_name, dataset_path, model_name, epoch, learning_rate, batch_size, weight_decay, device, save_dir):\n",
    "    device = torch.device(device)\n",
    "    dataset = get_dataset(dataset_name, dataset_path)\n",
    "    \n",
    "    print(\"dataset size : \", len(dataset))\n",
    "    \n",
    "    train_length = int(len(dataset) * 0.8)\n",
    "    valid_length = int(len(dataset) * 0.1)\n",
    "    test_length = len(dataset) - train_length - valid_length\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, (train_length, valid_length, test_length))\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=0)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    model = get_model(model_name, dataset).to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    early_stopper = EarlyStopper(num_trials=2, save_path=f'{save_dir}/{model_name}.pt')\n",
    "    for epoch_i in range(epoch):\n",
    "        train(model, optimizer, train_data_loader, criterion, device)\n",
    "        auc = test(model, valid_data_loader, device)\n",
    "        print('epoch:', epoch_i, 'validation: auc:', auc)\n",
    "        if not early_stopper.is_continuable(model, auc):\n",
    "            print(f'validation: best auc: {early_stopper.best_accuracy}')\n",
    "            break\n",
    "    auc = test(model, test_data_loader, device)\n",
    "    print(f'test auc: {auc}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset_name', default='avazu')\n",
    "    parser.add_argument('--dataset_path', default='../data/avazu/train.gz')\n",
    "    parser.add_argument('--model_name', default='ffm')\n",
    "    parser.add_argument('--epoch', type=int, default=100)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
    "    parser.add_argument('--device', default='cpu')\n",
    "    parser.add_argument('--save_dir', default='avazu/')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    main(args.dataset_name,\n",
    "        args.dataset_path,\n",
    "        args.model_name,\n",
    "        args.epoch,\n",
    "        args.learning_rate,\n",
    "        args.batch_size,\n",
    "        args.weight_decay,\n",
    "        args.device,\n",
    "        args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset('avazu', '../data/avazu/train.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset, batch_size=256, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69,   1,   5,  ...,  11,  35,  48],\n",
      "        [ 69,   1,   5,  ...,  11, 109,  48],\n",
      "        [ 69,   1,   5,  ...,  11, 109,  48],\n",
      "        ...,\n",
      "        [ 69,   1,   4,  ...,  24,  35,  14],\n",
      "        [ 69,   1,   4,  ...,  11,  35,  21],\n",
      "        [ 69,   1,   5,  ...,  51, 109,  54]])\n"
     ]
    }
   ],
   "source": [
    "for idx, vals  in train_data_loader:\n",
    "  print(idx)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('pytorch3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819822aad1d7dd5a8911f6442f86af1dcc8bcd014a1dc82c30af569169ee7dac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
